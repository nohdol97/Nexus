services:
  gateway:
    environment:
      GATEWAY_UPSTREAMS: "llama=http://vllm:8001"
      GATEWAY_DEFAULT_UPSTREAM: "llama"
    depends_on:
      - vllm

  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8001:8001"
    command: >-
      --model ${MODEL_ID:-meta-llama/Meta-Llama-3-8B-Instruct}
      --host 0.0.0.0
      --port 8001
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8001/v1/models', timeout=2)\"",
        ]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 30s
    shm_size: "1gb"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
